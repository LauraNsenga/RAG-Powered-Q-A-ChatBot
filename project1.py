# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XzeBb7FjUeOLEaxPjDtVUMp-CWILaMej

## Setting Up the Environment

First, we need to install the necessary libraries.
"""

!pip install streamlit transformers langchain faiss-cpu sentence-transformers langchain-community torch PyMuPDF pyngrok

print("‚úÖ All packages installed successfully!")

"""# Reset and Reconfigure ngrok

This notebook will:

1. **Kill existing ngrok processes** to avoid conflicts.
2. **Disconnect all active tunnels**.
3. **Delete the old ngrok configuration file** (`~/.ngrok2/ngrok.yml`).
4. **Set a new ngrok authentication token** so you can start fresh.


Once run, you'll have ngrok ready to open new tunnels without leftover settings or processes.

"""

from pyngrok import ngrok
import subprocess
import os

# Kill existing ngrok processes
try:
    subprocess.run(["pkill", "-f", "ngrok"], check=False)
    print("üîÑ Killed existing ngrok processes")
except:
    pass

# Disconnect all tunnels
try:
    ngrok.kill()
    print("üîå Disconnected all tunnels")
except:
    pass

# Clear ngrok config
ngrok_config_path = os.path.expanduser("~/.ngrok2/ngrok.yml")
if os.path.exists(ngrok_config_path):
    os.remove(ngrok_config_path)
    print("üóëÔ∏è Cleared old ngrok config")

# IMPORTANT: Replace with your actual ngrok token from step 1
NGROK_TOKEN = "317DzPv5yvjSLPjKVDhBmOCVKok_7s89d3r7a5gTN88BSztLC"

# Set ngrok auth token
ngrok.set_auth_token(NGROK_TOKEN)

print("‚úÖ Ngrok setup complete!")
print("üîó Make sure you replaced YOUR_NGROK_TOKEN_HERE with your actual token")

"""## Create the app.py file for Streamlit

This cell writes a **Streamlit application** to `app.py` that lets you ask questions about your school catalog using **Retrieval-Augmented Generation (RAG)** with the **Phi-3-mini-4k-instruct** model.

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain.llms import HuggingFacePipeline
# from langchain.vectorstores import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.chains import RetrievalQA
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# import fitz  # PyMuPDF
# import tempfile
# import os
# 
# # Page config
# st.set_page_config(
#     page_title="School Catalog RAG Chatbot",
#     page_icon="üéì",
#     layout="wide"
# )
# 
# # Title
# st.title("üéì School Catalog Q&A Chatbot")
# st.markdown("Upload your school catalog PDF and ask questions about it!")
# 
# def read_pdf(file_path):
#     """Extract text from PDF file"""
#     doc = fitz.open(file_path)
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     doc.close()
#     return text
# 
# @st.cache_resource
# def initialize_model():
#     """Initialize the Phi-3 model and tokenizer"""
#     with st.spinner("Loading Phi-3 model... This may take a few minutes on first run."):
#         try:
#             # Create pipeline with specific parameters for Phi-3
#             pipe = pipeline(
#                 "text-generation",
#                 model="microsoft/Phi-3-mini-4k-instruct",
#                 max_new_tokens=200,
#                 temperature=0.3,
#                 do_sample=True,
#                 pad_token_id=50256,
#                 return_full_text=False,
#                 device_map="auto" if torch.cuda.is_available() else None
#             )
# 
#             llm = HuggingFacePipeline(pipeline=pipe)
#             return llm
#         except Exception as e:
#             st.error(f"Error loading model: {str(e)}")
#             return None
# 
# @st.cache_resource
# def create_vectorstore(_texts):
#     """Create FAISS vector store from texts"""
#     with st.spinner("Creating embeddings..."):
#         try:
#             # Split text into chunks for better retrieval
#             text_splitter = RecursiveCharacterTextSplitter(
#                 chunk_size=1000,
#                 chunk_overlap=200,
#                 length_function=len
#             )
# 
#             # Split the text into chunks
#             chunks = []
#             for text in _texts:
#                 chunks.extend(text_splitter.split_text(text))
# 
#             # Create embeddings
#             embeddings = HuggingFaceEmbeddings(
#                 model_name="sentence-transformers/all-MiniLM-L6-v2"
#             )
# 
#             # Create vector store
#             docsearch = FAISS.from_texts(chunks, embeddings)
#             return docsearch
#         except Exception as e:
#             st.error(f"Error creating vector store: {str(e)}")
#             return None
# 
# def create_qa_chain(llm, vectorstore):
#     """Create the QA chain"""
#     try:
#         qa = RetrievalQA.from_chain_type(
#             llm=llm,
#             chain_type="stuff",
#             retriever=vectorstore.as_retriever(
#                 search_type="similarity",
#                 search_kwargs={"k": 3}
#             ),
#             return_source_documents=False
#         )
#         return qa
#     except Exception as e:
#         st.error(f"Error creating QA chain: {str(e)}")
#         return None
# 
# # Sidebar for file upload
# with st.sidebar:
#     st.header("üìÅ Upload PDF")
#     uploaded_file = st.file_uploader(
#         "Choose a PDF file",
#         type="pdf",
#         help="Upload your school catalog or handbook PDF"
#     )
# 
#     if uploaded_file is not None:
#         st.success(f"File uploaded: {uploaded_file.name}")
# 
#         # Save uploaded file temporarily
#         with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
#             tmp_file.write(uploaded_file.read())
#             tmp_file_path = tmp_file.name
# 
#         # Process PDF
#         with st.spinner("Processing PDF..."):
#             try:
#                 pdf_text = read_pdf(tmp_file_path)
#                 st.session_state['pdf_text'] = [pdf_text]
#                 st.session_state['pdf_processed'] = True
# 
#                 # Clean up temporary file
#                 os.unlink(tmp_file_path)
# 
#                 # Show some stats
#                 st.subheader("üìä Document Stats")
#                 st.metric("Characters", len(pdf_text))
#                 st.metric("Words (approx)", len(pdf_text.split()))
# 
#             except Exception as e:
#                 st.error(f"Error processing PDF: {str(e)}")
#                 st.session_state['pdf_processed'] = False
# 
#     st.divider()
# 
#     # Model info
#     st.subheader("ü§ñ Model Info")
#     st.info("**Model**: microsoft/Phi-3-mini-4k-instruct\\n**Purpose**: RAG-based Q&A")
# 
#     # GPU info
#     if torch.cuda.is_available():
#         st.success(f"üöÄ GPU Available: {torch.cuda.get_device_name()}")
#     else:
#         st.info("üíª Running on CPU")
# 
# # Initialize session state
# if 'pdf_processed' not in st.session_state:
#     st.session_state['pdf_processed'] = False
# 
# if 'messages' not in st.session_state:
#     st.session_state['messages'] = []
# 
# # Main interface
# if st.session_state['pdf_processed'] and 'pdf_text' in st.session_state:
# 
#     # Initialize model (cached)
#     llm = initialize_model()
# 
#     if llm is not None:
#         # Create vector store (cached)
#         vectorstore = create_vectorstore(st.session_state['pdf_text'])
# 
#         if vectorstore is not None:
#             # Create QA chain
#             qa_chain = create_qa_chain(llm, vectorstore)
# 
#             if qa_chain is not None:
#                 st.success("‚úÖ Chatbot is ready! Ask questions about your document.")
# 
#                 # Display chat history
#                 for message in st.session_state['messages']:
#                     with st.chat_message(message["role"]):
#                         st.markdown(message["content"])
# 
#                 # Chat input
#                 if prompt := st.chat_input("Ask a question about your document..."):
#                     # Add user message
#                     st.session_state['messages'].append({"role": "user", "content": prompt})
# 
#                     with st.chat_message("user"):
#                         st.markdown(prompt)
# 
#                     # Generate response
#                     with st.chat_message("assistant"):
#                         with st.spinner("Thinking..."):
#                             try:
#                                 # Use your original query format
#                                 result = qa_chain.invoke(prompt)
# 
#                                 # Extract the answer from the result
#                                 if isinstance(result, dict) and 'result' in result:
#                                     answer = result['result']
#                                 else:
#                                     answer = str(result)
# 
#                                 # Clean up the answer
#                                 answer = answer.strip()
# 
#                                 st.markdown(answer)
# 
#                                 # Add to chat history
#                                 st.session_state['messages'].append({
#                                     "role": "assistant",
#                                     "content": answer
#                                 })
# 
#                             except Exception as e:
#                                 error_msg = f"Sorry, I encountered an error: {str(e)}"
#                                 st.error(error_msg)
#                                 st.session_state['messages'].append({
#                                     "role": "assistant",
#                                     "content": error_msg
#                                 })
# 
#                 # Clear chat button
#                 if st.button("üóëÔ∏è Clear Chat"):
#                     st.session_state['messages'] = []
#                     st.rerun()
# 
#                 # Example questions
#                 with st.expander("üí° Example Questions"):
#                     st.markdown("""
#                     Try asking questions like:
#                     - Who do the attendance policies and penalties rest under?
#                     - What are the graduation requirements?
#                     - What are the school hours?
#                     - How do I contact the administration?
#                     - What are the disciplinary procedures?
#                     """)
#             else:
#                 st.error("Failed to create QA chain. Please try again.")
#         else:
#             st.error("Failed to create vector store. Please try again.")
#     else:
#         st.error("Failed to load the language model. Please check and try again.")
# 
# else:
#     # Instructions when no PDF is uploaded
#     col1, col2, col3 = st.columns([1, 2, 1])
# 
#     with col2:
#         st.info("""
#         ### How to get started:
# 
#         1. **Upload your PDF** in the sidebar
#         2. **Wait for processing** (first-time model loading may take a few minutes)
#         3. **Ask questions** about the content
# 
#         The chatbot will answer based only on the information in your uploaded document.
#         """)
# 
# # Footer
# st.divider()
# st.markdown("""
# <div style='text-align: center; color: #666; font-size: 0.8rem;'>
#     RAG Chatbot powered by Phi-3-mini-4k-instruct and LangChain | Running on Google Colab
# </div>
# """, unsafe_allow_html=True)

"""## Run Streamlit in the Background"""

import subprocess
import threading
import time

def run_streamlit():
    subprocess.run(["streamlit", "run", "app.py", "--server.port", "8501", "--server.headless", "true"])

# Start Streamlit in background
streamlit_thread = threading.Thread(target=run_streamlit)
streamlit_thread.daemon = True
streamlit_thread.start()

print("üöÄ Starting Streamlit server...")
print("‚è±Ô∏è  Please wait 10-15 seconds...")
time.sleep(10)

"""## Create public URL for Streamlit"""

# Create public tunnel
public_url = ngrok.connect(8501)
print(f"üåê Your Streamlit app is live at: {public_url}")
print(f"üîó Click the link above to access your chatbot!")
print(f"üì± You can also share this URL with others")
print()
print("üìù Instructions:")
print("1. Click the public URL above")
print("2. Your PDF is already processed!")
print("3. Start asking questions immediately!")
print()
print("‚ö†Ô∏è  Keep this Colab notebook running to maintain the connection")

import time

print("üîÑ Keeping the server alive...")
print("üìä Server status: Running")
print("üåê Public URL:", public_url)
print()
print("üí° Your chatbot is ready to use!")
print("üìã Document already processed and loaded")
print("üéØ You can start asking questions immediately")
print()
print("To stop the server, interrupt this cell or restart the runtime")
print("‚è∞ Server will run until you stop this cell...")

try:
    while True:
        time.sleep(30)  # Check every 30 seconds
        print(".", end="", flush=True)  # Show it's still running
except KeyboardInterrupt:
    print("\nüõë Server stopped by user")
    ngrok.disconnect(public_url)
    print("üîå Ngrok tunnel closed")